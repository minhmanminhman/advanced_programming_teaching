{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_sine_simulation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0hKMGaVHm3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pip install wand\n",
        "!apt-get install libmagickwand-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciMd4XbHFn-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(2)\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Generate training dataset\n",
        "T = 20\n",
        "L = 1000\n",
        "N = 100\n",
        "\n",
        "x = np.empty((N, L), 'int64')\n",
        "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
        "data = np.sin(x / 1.0 / T).astype('float64')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9uyIWmZFriU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Sequence(nn.Module):\n",
        "  \"\"\"\n",
        "  Define LTSM model to predict. There are 2 LSTM layers and 1 linear layer\n",
        "  input -> LSTM -> LSTM -> Linear -> output\n",
        "  Output of LSTM haing size 51 to capture the frequency of sine function,\n",
        "  then these features are pushed into linear layer to predict 1 next value\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(Sequence, self).__init__()\n",
        "    self.lstm1 = nn.LSTMCell(1, 51)\n",
        "    self.lstm2 = nn.LSTMCell(51, 51)\n",
        "    self.linear = nn.Linear(51, 1)\n",
        "  \n",
        "  def forward(self, input, future = 0):\n",
        "      outputs = []\n",
        "      h_t = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "      c_t = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "      h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "      c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double).to(device)\n",
        "\n",
        "      for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "          h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
        "          h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "          output = self.linear(h_t2)\n",
        "          outputs += [output]\n",
        "      for i in range(future):# if we should predict the future\n",
        "          h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
        "          h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
        "          output = self.linear(h_t2)\n",
        "          outputs += [output]\n",
        "      outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "      return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQWAdOn5H3DU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seed to 0\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "# load data and make training set\n",
        "input = torch.from_numpy(data[3:, :-1]).to(device)\n",
        "target = torch.from_numpy(data[3:, 1:]).to(device)\n",
        "test_input = torch.from_numpy(data[:3, :-1]).to(device)\n",
        "test_target = torch.from_numpy(data[:3, 1:]).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgPwZy_0KOCm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c99ccab-080e-4aa6-e246-0fee83de6896"
      },
      "source": [
        "input.shape, target.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([97, 999]), torch.Size([97, 999]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6I3puE4IBv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa606d00-d6a8-451c-fa17-2b9cab229307"
      },
      "source": [
        "# build the model\n",
        "seq = Sequence().to(device)\n",
        "seq.double()\n",
        "criterion = nn.MSELoss()\n",
        "# use LBFGS as optimizer since we can load the whole data to train\n",
        "optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
        "#begin to train\n",
        "for i in range(9):\n",
        "    print('STEP: ', i)\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        out = seq(input)\n",
        "        loss = criterion(out, target)\n",
        "        print('loss:', loss.item())\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    optimizer.step(closure)\n",
        "    # begin to predict, no need to track gradient here\n",
        "    with torch.no_grad():\n",
        "        future = 1000\n",
        "        pred = seq(test_input, future=future)\n",
        "        loss = criterion(pred[:, :-future], test_target)\n",
        "        print('test loss:', loss.item())\n",
        "        y = pred.cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STEP:  0\n",
            "loss: 0.5232618273269376\n",
            "loss: 0.5102756077161258\n",
            "loss: 0.4808570375818254\n",
            "loss: 0.468535563280216\n",
            "loss: 0.4259845377799631\n",
            "loss: 0.5157926443824742\n",
            "loss: 0.2704566437379761\n",
            "loss: 0.11931644973071746\n",
            "loss: 3.4599193733829376\n",
            "loss: 0.043432640399244035\n",
            "loss: 0.029299380139148262\n",
            "loss: 0.02449449696043418\n",
            "loss: 0.02377544421986648\n",
            "loss: 0.023092435314603615\n",
            "loss: 0.02153848495984922\n",
            "loss: 0.018752435249141223\n",
            "loss: 0.014304672296588503\n",
            "loss: 0.006829892151179761\n",
            "loss: 0.0055648657380485056\n",
            "loss: 0.004680194990260141\n",
            "test loss: 0.0025366800830563647\n",
            "STEP:  1\n",
            "loss: 0.002596144268814951\n",
            "loss: 0.0020321605738842577\n",
            "loss: 0.0018123996560673742\n",
            "loss: 0.0017216475960275916\n",
            "loss: 0.001547140222974879\n",
            "loss: 0.0012611929244023745\n",
            "loss: 0.0009823830180296562\n",
            "loss: 0.0008383725311573748\n",
            "loss: 0.00083007702211389\n",
            "loss: 0.000814618357658919\n",
            "loss: 0.0007883118096878153\n",
            "loss: 0.0007370170318109564\n",
            "loss: 0.0006537767217807537\n",
            "loss: 0.0005423691258222698\n",
            "loss: 0.0004326284270049619\n",
            "loss: 0.0003967479903333224\n",
            "loss: 0.0003909070951420419\n",
            "loss: 0.0003812451106444941\n",
            "loss: 0.0003786343774109635\n",
            "loss: 0.0003762900852135547\n",
            "test loss: 0.0002434346867545713\n",
            "STEP:  2\n",
            "loss: 0.00037453845429962786\n",
            "loss: 0.00037308720114841257\n",
            "loss: 0.0003716591004981104\n",
            "loss: 0.0003690220374157154\n",
            "loss: 0.00036370203127980696\n",
            "loss: 0.0003536463961002943\n",
            "loss: 0.0003387086484589443\n",
            "loss: 0.00031937892921076816\n",
            "loss: 0.00028343750359278015\n",
            "loss: 0.00024310892365971888\n",
            "loss: 0.0002062574722988123\n",
            "loss: 0.00019855701359107166\n",
            "loss: 0.00019324229342486147\n",
            "loss: 0.0001908837574099141\n",
            "loss: 0.00018871430412375744\n",
            "loss: 0.00018815224325702512\n",
            "loss: 0.00018696980543953892\n",
            "loss: 0.00018218003029750346\n",
            "loss: 0.00017410981022896404\n",
            "loss: 0.0001639213927044309\n",
            "test loss: 6.156767695969169e-05\n",
            "STEP:  3\n",
            "loss: 0.0001636367815609301\n",
            "loss: 0.00015054582357787436\n",
            "loss: 0.00014687721954946586\n",
            "loss: 0.00013957936181843733\n",
            "loss: 0.00012714497269805496\n",
            "loss: 0.000123606597472272\n",
            "loss: 0.0001188559740897552\n",
            "loss: 0.00011857504922763459\n",
            "loss: 0.00011842824912464315\n",
            "loss: 0.00011831233793248136\n",
            "loss: 0.00011753772285482509\n",
            "loss: 0.00011331811820770124\n",
            "loss: 0.00010807783123583224\n",
            "loss: 0.00010157855400515731\n",
            "loss: 0.00011185841642413431\n",
            "loss: 9.458728635732373e-05\n",
            "loss: 9.226363321833007e-05\n",
            "loss: 8.812602018517539e-05\n",
            "loss: 7.767430287551934e-05\n",
            "loss: 7.224452939302076e-05\n",
            "test loss: 4.1147807882208286e-05\n",
            "STEP:  4\n",
            "loss: 6.980786544253401e-05\n",
            "loss: 6.815529751929227e-05\n",
            "loss: 6.739594218988976e-05\n",
            "loss: 6.711344890090764e-05\n",
            "loss: 6.689819658779376e-05\n",
            "loss: 6.645263306041779e-05\n",
            "loss: 6.563544956566778e-05\n",
            "loss: 6.39878358758001e-05\n",
            "loss: 6.0990832989682565e-05\n",
            "loss: 0.00012644403693355978\n",
            "loss: 5.514213112845307e-05\n",
            "loss: 5.407537712818784e-05\n",
            "loss: 5.179418471967274e-05\n",
            "loss: 5.060221523437204e-05\n",
            "loss: 4.9561735742337004e-05\n",
            "loss: 4.746972843204757e-05\n",
            "loss: 4.634282566242427e-05\n",
            "loss: 4.5272099998448794e-05\n",
            "loss: 4.456364373887548e-05\n",
            "loss: 4.384591967557081e-05\n",
            "test loss: 3.4578190670667236e-05\n",
            "STEP:  5\n",
            "loss: 4.325122807649551e-05\n",
            "loss: 4.2875944088664544e-05\n",
            "loss: 4.232626256807479e-05\n",
            "loss: 4.1507758151599925e-05\n",
            "loss: 4.001964014514467e-05\n",
            "loss: 3.984748988427781e-05\n",
            "loss: 3.830997327422967e-05\n",
            "loss: 3.7738988444461e-05\n",
            "loss: 3.713987129240261e-05\n",
            "loss: 3.6563066238820054e-05\n",
            "loss: 3.555952468385857e-05\n",
            "loss: 3.439738674927408e-05\n",
            "loss: 3.3275835162361843e-05\n",
            "loss: 3.25710021391471e-05\n",
            "loss: 3.235537412929996e-05\n",
            "loss: 3.209174105843972e-05\n",
            "loss: 3.1986303965092187e-05\n",
            "loss: 3.1807355777208105e-05\n",
            "loss: 3.1606641604923875e-05\n",
            "loss: 3.1560410004315434e-05\n",
            "test loss: 3.05106194725131e-05\n",
            "STEP:  6\n",
            "loss: 3.152872078247116e-05\n",
            "loss: 3.146280009860194e-05\n",
            "loss: 3.13382264009427e-05\n",
            "loss: 3.108041072763797e-05\n",
            "loss: 3.050504646706542e-05\n",
            "loss: 2.9364176084234655e-05\n",
            "loss: 2.7631494025740804e-05\n",
            "loss: 2.5744479312974534e-05\n",
            "loss: 2.5066572529212757e-05\n",
            "loss: 2.9208532149447282e-05\n",
            "loss: 2.2588733974026838e-05\n",
            "loss: 2.1528819249252565e-05\n",
            "loss: 2.005524363968049e-05\n",
            "loss: 1.916076631824215e-05\n",
            "loss: 1.8432526693627767e-05\n",
            "loss: 1.797487628582774e-05\n",
            "loss: 1.75380225470108e-05\n",
            "loss: 1.7127259231226888e-05\n",
            "loss: 1.6416157263753483e-05\n",
            "loss: 1.59056349314856e-05\n",
            "test loss: 1.7492100427984886e-05\n",
            "STEP:  7\n",
            "loss: 1.4892997048342145e-05\n",
            "loss: 1.4575707819615403e-05\n",
            "loss: 1.4313203110346855e-05\n",
            "loss: 1.4200360291910052e-05\n",
            "loss: 1.4118608583636203e-05\n",
            "loss: 1.4064128686132782e-05\n",
            "loss: 1.3975083965511899e-05\n",
            "loss: 1.3843884767379677e-05\n",
            "loss: 1.3676551026672064e-05\n",
            "loss: 1.3530335695291083e-05\n",
            "loss: 1.3361475983438671e-05\n",
            "loss: 2.5326138625339565e-05\n",
            "loss: 1.3170440722040937e-05\n",
            "loss: 1.3026185348797364e-05\n",
            "loss: 1.2928504701407171e-05\n",
            "loss: 1.2744991396381414e-05\n",
            "loss: 1.2496343737815582e-05\n",
            "loss: 1.247202920951233e-05\n",
            "loss: 1.234985130914464e-05\n",
            "loss: 1.2259722340554964e-05\n",
            "test loss: 1.4983927990775825e-05\n",
            "STEP:  8\n",
            "loss: 1.2036873271871963e-05\n",
            "loss: 1.1793029345282239e-05\n",
            "loss: 1.159060645324366e-05\n",
            "loss: 1.1306560369681899e-05\n",
            "loss: 1.0901353814105751e-05\n",
            "loss: 1.0387253444412515e-05\n",
            "loss: 1.0117499192000465e-05\n",
            "loss: 1.0038556386012894e-05\n",
            "loss: 1.002127975680506e-05\n",
            "loss: 9.985315697297009e-06\n",
            "loss: 9.944157482671042e-06\n",
            "loss: 9.72945450321828e-06\n",
            "loss: 9.56072212931099e-06\n",
            "loss: 9.438278396222689e-06\n",
            "loss: 9.159249647984527e-06\n",
            "loss: 8.939000525399348e-06\n",
            "loss: 8.803593407504702e-06\n",
            "loss: 8.755459609429642e-06\n",
            "loss: 8.724229362223589e-06\n",
            "loss: 8.711702278129569e-06\n",
            "test loss: 1.2555843475858367e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co3jx0nVn5Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw the result\n",
        "plt.figure(figsize=(30,10))\n",
        "plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
        "plt.xlabel('x', fontsize=20)\n",
        "plt.ylabel('y', fontsize=20)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "def draw(yi, color):\n",
        "      plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
        "      plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
        "draw(y[0], 'r')\n",
        "draw(y[1], 'g')\n",
        "draw(y[2], 'b')\n",
        "plt.savefig('predict%d.pdf'%i)\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzyzNFQcryGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "d99509db-6586-4872-de6c-3ce8ecc65959"
      },
      "source": [
        "from wand.image import Image as WImage\n",
        "img = WImage(filename='predict8.pdf')\n",
        "img"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PolicyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPolicyError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9becf294591b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mWImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict8.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wand/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image, blob, file, filename, format, width, height, depth, background, resolution, pseudo, colorspace, units)\u001b[0m\n\u001b[1;32m   8553\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8554\u001b[0m                     self.read(filename=filename, resolution=resolution,\n\u001b[0;32m-> 8555\u001b[0;31m                               units=units)\n\u001b[0m\u001b[1;32m   8556\u001b[0m                 \u001b[0;31m# clear the wand format, otherwise any subsequent call to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8557\u001b[0m                 \u001b[0;31m# MagickGetImageBlob will silently change the image to this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wand/image.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file, filename, blob, resolution, units)\u001b[0m\n\u001b[1;32m   9028\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMagickReadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9029\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9030\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9031\u001b[0m             msg = ('MagickReadImage returns false, but did not raise '\n\u001b[1;32m   9032\u001b[0m                    \u001b[0;34m'ImageMagick  exception. This can occur when a delegate '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wand/resource.py\u001b[0m in \u001b[0;36mraise_exception\u001b[0;34m(self, stacklevel)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_blob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPolicyError\u001b[0m: not authorized `predict8.pdf' @ error/constitute.c/ReadImage/412"
          ]
        }
      ]
    }
  ]
}