{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "Experience = namedtuple(\"Experience\", ['state', 'action', 'reward', 'next_state', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class REINFORCEBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    Create policy network which takes state featues as input and outputs state values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(REINFORCEBaseline, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x_policy = self.policy_head(x)\n",
    "        x_value = self.value_head(x)\n",
    "        return x_policy, x_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_policy_values(model, state):\n",
    "    \"\"\"\n",
    "    Calculate unnormalized policy values in a state.\n",
    "    Args:\n",
    "        state: a numpy array containing state features \n",
    "    Returns:\n",
    "        a tensor of unnormalized policy values \n",
    "    \"\"\"\n",
    "    state = Variable(torch.from_numpy(state)).type(torch.FloatTensor).unsqueeze(0)\n",
    "    policy_values, state_value = model(state)\n",
    "    return policy_values, state_value\n",
    "\n",
    "def generate_episode(env, policy):\n",
    "    \"\"\"\n",
    "    Return experience in an episode\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    s = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        policy_values, _ = get_policy_values(policy, s)\n",
    "        action_probs = F.softmax(policy_values, dim=-1).detach().numpy().reshape(-1)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "\n",
    "        episode.append(Experience(s, action, r, next_s, done))\n",
    "        \n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def update_model(episode, policy, optimizer):\n",
    "    policy_loss, value_loss = 0, 0\n",
    "    value_loss_func = nn.MSELoss()\n",
    "    for t, exp in enumerate(episode):\n",
    "        k = len(episode[t:])\n",
    "        policy_values, state_value = get_policy_values(policy, exp.state)\n",
    "        #============UPDATE VALUE FUNCTION STEP============\n",
    "        discount_reward = np.sum(GAMMA ** np.arange(k) * np.ones(k))\n",
    "        true_reward = torch.FloatTensor(discount_reward.reshape(-1,1))\n",
    "        value_loss += value_loss_func(true_reward, state_value)\n",
    "        \n",
    "        #============UPDATE POLICY STEP=================\n",
    "        baseline = state_value.reshape(-1)\n",
    "        log_probs = F.log_softmax(policy_values).reshape(-1)\n",
    "        policy_target = discount_reward - baseline\n",
    "        policy_loss += -log_probs[exp.action] * policy_target * GAMMA ** t\n",
    "#         print(policy_loss)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = value_loss + policy_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def predict_action(model, state):\n",
    "    policy_values, _ = get_policy_values(model, s)\n",
    "    action_probs = F.softmax(policy_values, dim=-1).detach().numpy().reshape(-1)\n",
    "    action = np.argmax(action_probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR = 0.003\n",
    "GAMMA = 0.99\n",
    "EPISODES = 1000\n",
    "NUM_TEST = 20\n",
    "\n",
    "s = env.reset()\n",
    "model = REINFORCEBaseline()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for ep in range(EPISODES):\n",
    "    episode = generate_episode(env, model)\n",
    "    update_model(episode, model, optimizer)\n",
    "    \n",
    "    # Test agent\n",
    "    list_reward = []\n",
    "    if ep % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            for n_test in range(NUM_TEST):\n",
    "                episode = generate_episode(env, model)\n",
    "                list_reward.append((len(episode)))\n",
    "            mean_reward = np.mean(list_reward)\n",
    "            print(f\"Episode: {ep}. Mean reward: {mean_reward}\")\n",
    "    if mean_reward > 199:\n",
    "        print(f\"Game solved in {ep} episode(s)!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'reinfoce_baseline.pth')\n",
    "model = torch.load(\"reinfoce_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "# Use Wrapper.Moniter from openai gym to record video\n",
    "wrapped_env = wrappers.Monitor(env, directory='./videos/reinforce_baseline_cartpole_1', force=True, resume=True)\n",
    "s = wrapped_env.reset()\n",
    "while True:\n",
    "    action = predict_action(model, s)\n",
    "    s, _, done, _ = wrapped_env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "wrapped_env.close()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 413.79999999999995,
   "position": {
    "height": "40px",
    "left": "1266px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
