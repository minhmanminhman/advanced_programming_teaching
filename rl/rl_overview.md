Introduction
====================

Tabular Solution Methods
=========================

## Dynamic Programming

### Policy Iteration

### Value Iteration

![Value Iteration Algorithm](VI_algo.PNG "Value Iteration Algorithm")

With model-based assumption, we know every state and transition probabilities (or we can learn transition probabilities). Each state value implies the highest average reward that the agent can get at that state. In order to get the highest reward, we computes the value function by iterating over every state in the enviroment until convergence. The policy is then extracted from the optimal value function by taking actions leading to highest reward in average.

## Temporal-difference Learning

Temporal-difference is a class of model-free algorithm. **Model-free** means that the agent doesn't know anything about the enviroment such as states, transition probability. It will experience the enviroment (data) by itself to find the best way to interact to get highest reward. But how will the agent interact with the enviroment? There are 2 main ways:

- On-policy: data is generated by a given policy. The agent will improve the given policy to obtain optimal policy
- Off-policy: data is generated without a given policy. Instead, the agent will act according to the **value function** or **q function** to generate data. To make sure the agent will explore the enviroment, **$\epsilon$-greedy policy** is often used.

### SARSA

### Q-learning

Q-learning is a off-policy TD algorithm.

![Value Iteration Algorithm](Q_learning.PNG "Value Iteration Algorithm")

Approximation Solution Methods
=========================

## Policy Gradient Method

### REINFORCE

### Actor-Critic

## Deep Q-learning (DQN)
